# @package _global_

###########################
### Classification with Hierarchical Attention Network 
### Moving predictions

defaults:
  - /datamodule: moving_set_min_emp_tab
  - /trainer: moving
  - /callbacks
  
#GENERAL
version: 0.1
implementation: "hyperparams-moving-fnn-min-emp-final"
comments: "tuning"
name: "run8"
model_name: "run8"
seed: 2023

pretrained_model_path: "master_thesis/checkpoints/hyperparams-moving-fnn-min-emp-final/run8/run8/0.1/best.ckpt"
stage: "finetuning" # pre_training, finetuning, prediction, hyp_tune
cls_num_targets: 2
num_targets: 2
pooled: False #if the final representation is an averaged version of the sequence
ckpt_path: ${last_ckpt:${callbacks.checkpoint_cls.dirpath}} ##a bit weird implementation - fix later
callbacks.checkpoint.dirpath: ${ckpt_path}

###########################
###################### MODEL
model:
  _target_: src.models.ffn.FFN
  _convert_: all
  hparams:


    input_size: 985 #this one you should knwo in advance (fix later) ## NB CHANGE VALUE
    hidden_size: 64
    n_layers: 1
    dropout: 0.002
    batch_size: ${datamodule.batch_size}

    encoder_type: 'neural' #or 'logistic'


    #### CLS SPECIFIC
    cls_num_targets: ${cls_num_targets}
    num_targets: ${num_targets}
    loss_type: "entropy"
    average_type: 'micro'
    #class_weights: [1/0.781, 1/0.219]
    class_weights: [1.2804, 4.5662]  

    learning_rate: 0.008
    weight_decay: 1.e-4
    beta1: 0.9
    beta2: 0.999
    adamsgrad: True
    epsilon: 1.e-6
    training_task: ${datamodule.task.name} 
    stage: ${stage}
    implementation: ${implementation}
    version: ${version}
    experiment_name: ${name}_${model_name}
    experiment_version: ${version} 

trainer:
  accelerator: 'gpu'
  devices: [0]
  callbacks:
  - ${callbacks.checkpoint_cls}
  - ${callbacks.lr_monitor}
  - ${callbacks.silence_warnings}
  - ${callbacks.reseed_dataloader}
  - ${callbacks.early_stopping}
  - ${callbacks.track_ids}
  - ${callbacks.collect_outputs} 