# @package _global_

###################################
#### Classification with company2vec model 
#### Moving predictions

defaults:
  - /datamodule: moving_set_min_emp
  - /trainer: moving
  - /callbacks
  
#GENERAL
version: 0.1
implementation: "hyperparams-moving-rnn-min-emp-final"
comments: "Final"
name: "run2"
model_name: "run2"
seed: 2023

stage: "prediction" # pre_training, finetuning, prediction, hyp_tune

### dissable during the tuning
pretrained_model_path: "147319/master_thesis/checkpoints/hyperparams-moving-rnn-min-emp-final/run2/run2/0.1/best.ckpt"
ckpt_path: ${last_ckpt:${callbacks.checkpoint_cls.dirpath}} ##a bit weird implementation - fix later
callbacks.checkpoint.dirpath: ${ckpt_path}

###########################
###################### MODEL[370, 1, 0.16, False]
model:
  _target_: src.models.rnn.SimpleGRU
  _convert_: all
  hparams:
    seed: ${seed}
    vocab_size: 993 #this one you should knwo in advance (fix later) ## NB CHANGE VALUE
    batch_size: ${datamodule.batch_size}
    max_length: ${datamodule.task.max_length}    
    hidden_size: 64
    hidden_ff: -1
    hidden_act: "swish"
    n_encoders: -1
    n_heads: -1
    n_local: -1
    local_window_size: -1
    norm_type: None
    att_dropout: 0.05 ## Attention Layers
    fw_dropout: 0.05 ## Positionwise Layers
    dc_dropout: 0.05 ## Decoder Layer
    emb_dropout: 0.05 ## Embedding dropout
    ## TASK AND LOGS
    training_task: ${datamodule.task.name} # name of the task [mlm, simple]
    experiment_name: ${name}_${model_name}
    experiment_version: ${version} 
    ## ATTENTION
    attention_type: None
    multihead_dc: None
    num_random_features: -1

    parametrize_emb: True
    norm_input_emb: False
    norm_output_emb: False
    freeze_embeddings: False
    freeze_positions: False

    ## OPTIMIZER
    optimizer_type: "radam"
    learning_rate: 1e-3
    weight_decay: 1e-3
    beta1: 0.9
    beta2: 0.999
    lr_gamma: 0.8
    epsilon: 1.e-8

    ## RNN
    n_layers: 4
    bidirectional: true

    #### CLS SPECIFIC

    num_targets: 2
    cls_num_targets: 2
    loss_type: "entropy"
    average_type: 'micro'
    #class_weights: [1/0.781, 1/0.219]
    class_weights: [1.2804, 4.5662]  

    pooled: True
    num_pooled_sep: ${datamodule.task.num_pooled_sep}
    pretrained_model_path: None

    stage: ${stage}
    implementation: ${implementation}
    version: ${version}


trainer:
  accelerator: 'gpu' # 'cpu'
  devices: [0] # 1
  callbacks:
    - ${callbacks.checkpoint_cls}
    - ${callbacks.lr_monitor}
    - ${callbacks.silence_warnings}
    - ${callbacks.reseed_dataloader}
    - ${callbacks.early_stopping}
    - ${callbacks.save_weights}
    - ${callbacks.collect_outputs}
    - ${callbacks.track_ids}